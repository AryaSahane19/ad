{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0328e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A     B    C     D  E\n",
      "0  52.0   4.0  4.0  40.5  X\n",
      "1  93.0  89.0  NaN  50.5  X\n",
      "2  15.0  60.0  4.0  50.5  X\n",
      "3  72.0  14.0  5.0  30.5  X\n",
      "4  61.0   9.0  1.0   NaN  Z\n"
     ]
    }
   ],
   "source": [
    "#Missing values dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) # Set a seed for reproducibility\n",
    "\n",
    "# Create a DataFrame with 50 rows and 5 columns\n",
    "data = {\n",
    "    'A': np.random.randint(1, 100, size=50),\n",
    "    'B': np.random.randint(1, 100, size=50),\n",
    "    'C': np.random.choice([1, 2, 3, 4, 5], size=50),\n",
    "    'D': np.random.choice([10.5, 20.5, 30.5, 40.5, 50.5], size=50),\n",
    "    'E': np.random.choice(['X', 'Y', 'Z'], size=50)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce random missing values (NaNs) in columns A, B, and C\n",
    "missing_indices = np.random.choice(df.index, size=15, replace=False)  \n",
    "# Randomly select 15 rows to be missing\n",
    "df.loc[missing_indices, 'A'] = np.nan\n",
    "df.loc[missing_indices, 'B'] = np.nan\n",
    "\n",
    "# Introduce NaNs in a few other random spots\n",
    "df.loc[np.random.choice(df.index, size=5, replace=False), 'C'] = np.nan\n",
    "df.loc[np.random.choice(df.index, size=5, replace=False), 'D'] = np.nan\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('missing_values_dataset.csv', index=False)\n",
    "\n",
    "# Print the first few rows to see what it looks like\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a608738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset with missing values:\n",
      "       A     B    C     D  E\n",
      "0   52.0   4.0  4.0  40.5  X\n",
      "1   93.0  89.0  NaN  50.5  X\n",
      "2   15.0  60.0  4.0  50.5  X\n",
      "3   72.0  14.0  5.0  30.5  X\n",
      "4   61.0   9.0  1.0   NaN  Z\n",
      "5   21.0  90.0  5.0  40.5  X\n",
      "6   83.0  53.0  5.0  50.5  Y\n",
      "7   87.0   2.0  1.0  30.5  Y\n",
      "8   75.0  84.0  1.0  30.5  Y\n",
      "9    NaN   NaN  NaN  40.5  Z\n",
      "10  88.0  60.0  1.0  20.5  X\n",
      "11  24.0  71.0  4.0  20.5  X\n",
      "12   NaN   NaN  NaN  50.5  X\n",
      "13   NaN   NaN  3.0  10.5  Z\n",
      "14   NaN   NaN  1.0  50.5  Y\n",
      "15   2.0  35.0  3.0  40.5  Y\n",
      "16  88.0  78.0  3.0  40.5  X\n",
      "17   NaN   NaN  NaN  40.5  Y\n",
      "18  38.0  36.0  3.0   NaN  Y\n",
      "19   2.0  50.0  5.0  40.5  Z\n",
      "20   NaN   NaN  2.0   NaN  Z\n",
      "21  60.0   2.0  2.0  20.5  Z\n",
      "22  21.0   6.0  1.0  40.5  Z\n",
      "23  33.0  54.0  4.0  10.5  X\n",
      "24  76.0   4.0  1.0  10.5  Z\n",
      "25  58.0  54.0  4.0  10.5  Y\n",
      "26   NaN   NaN  2.0  10.5  X\n",
      "27  89.0  63.0  1.0   NaN  Y\n",
      "28  49.0  18.0  NaN  10.5  Y\n",
      "29  91.0  90.0  3.0  40.5  Y\n",
      "30  59.0  44.0  4.0  50.5  Z\n",
      "31  42.0  34.0  3.0  10.5  Z\n",
      "32  92.0  74.0  3.0  30.5  X\n",
      "33   NaN   NaN  1.0  30.5  X\n",
      "34  80.0  14.0  3.0  10.5  Z\n",
      "35   NaN   NaN  5.0  50.5  Y\n",
      "36   NaN   NaN  3.0  10.5  X\n",
      "37  62.0  15.0  1.0  30.5  Z\n",
      "38   NaN   NaN  5.0  20.5  Z\n",
      "39  62.0  78.0  2.0  40.5  Z\n",
      "40  51.0  87.0  3.0  30.5  Y\n",
      "41  55.0  62.0  1.0  10.5  Z\n",
      "42  64.0  40.0  2.0   NaN  Z\n",
      "43   NaN   NaN  2.0  10.5  Z\n",
      "44   NaN   NaN  4.0  10.5  Z\n",
      "45   NaN   NaN  5.0  20.5  X\n",
      "46  21.0  53.0  3.0  40.5  X\n",
      "47   NaN   NaN  1.0  40.5  Z\n",
      "48  39.0  26.0  4.0  20.5  Y\n",
      "49  18.0  89.0  5.0  30.5  X\n",
      "\n",
      "Dataset after imputation:\n",
      "            A     B    C     D  E\n",
      "0   52.000000   4.0  4.0  40.5  X\n",
      "1   93.000000  89.0  1.0  50.5  X\n",
      "2   15.000000  60.0  4.0  50.5  X\n",
      "3   72.000000  14.0  5.0  30.5  X\n",
      "4   61.000000   9.0  1.0   NaN  Z\n",
      "5   21.000000  90.0  5.0  40.5  X\n",
      "6   83.000000  53.0  5.0  50.5  Y\n",
      "7   87.000000   2.0  1.0  30.5  Y\n",
      "8   75.000000  84.0  1.0  30.5  Y\n",
      "9   54.942857  53.0  1.0  40.5  Z\n",
      "10  88.000000  60.0  1.0  20.5  X\n",
      "11  24.000000  71.0  4.0  20.5  X\n",
      "12  54.942857  53.0  1.0  50.5  X\n",
      "13  54.942857  53.0  3.0  10.5  Z\n",
      "14  54.942857  53.0  1.0  50.5  Y\n",
      "15   2.000000  35.0  3.0  40.5  Y\n",
      "16  88.000000  78.0  3.0  40.5  X\n",
      "17  54.942857  53.0  1.0  40.5  Y\n",
      "18  38.000000  36.0  3.0   NaN  Y\n",
      "19   2.000000  50.0  5.0  40.5  Z\n",
      "20  54.942857  53.0  2.0   NaN  Z\n",
      "21  60.000000   2.0  2.0  20.5  Z\n",
      "22  21.000000   6.0  1.0  40.5  Z\n",
      "23  33.000000  54.0  4.0  10.5  X\n",
      "24  76.000000   4.0  1.0  10.5  Z\n",
      "25  58.000000  54.0  4.0  10.5  Y\n",
      "26  54.942857  53.0  2.0  10.5  X\n",
      "27  89.000000  63.0  1.0   NaN  Y\n",
      "28  49.000000  18.0  1.0  10.5  Y\n",
      "29  91.000000  90.0  3.0  40.5  Y\n",
      "30  59.000000  44.0  4.0  50.5  Z\n",
      "31  42.000000  34.0  3.0  10.5  Z\n",
      "32  92.000000  74.0  3.0  30.5  X\n",
      "33  54.942857  53.0  1.0  30.5  X\n",
      "34  80.000000  14.0  3.0  10.5  Z\n",
      "35  54.942857  53.0  5.0  50.5  Y\n",
      "36  54.942857  53.0  3.0  10.5  X\n",
      "37  62.000000  15.0  1.0  30.5  Z\n",
      "38  54.942857  53.0  5.0  20.5  Z\n",
      "39  62.000000  78.0  2.0  40.5  Z\n",
      "40  51.000000  87.0  3.0  30.5  Y\n",
      "41  55.000000  62.0  1.0  10.5  Z\n",
      "42  64.000000  40.0  2.0   NaN  Z\n",
      "43  54.942857  53.0  2.0  10.5  Z\n",
      "44  54.942857  53.0  4.0  10.5  Z\n",
      "45  54.942857  53.0  5.0  20.5  X\n",
      "46  21.000000  53.0  3.0  40.5  X\n",
      "47  54.942857  53.0  1.0  40.5  Z\n",
      "48  39.000000  26.0  4.0  20.5  Y\n",
      "49  18.000000  89.0  5.0  30.5  X\n",
      "\n",
      "Updated dataset saved as 'imputed_missing_values_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('missing_values_dataset.csv') \n",
    "# Display the original dataset with missing values\n",
    "print(\"Original dataset with missing values:\")\n",
    "print(df)\n",
    "\n",
    "# Impute missing values for column A with mean\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "\n",
    "# Impute missing values for column B with median\n",
    "df['B'].fillna(df['B'].median(), inplace=True)\n",
    "\n",
    "# Impute missing values for column C with mode\n",
    "df['C'].fillna(df['C'].mode()[0], inplace=True)\n",
    "\n",
    "# Display the dataset after imputation\n",
    "print(\"\\nDataset after imputation:\")\n",
    "print(df)\n",
    "#print(df.head())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('imputed_missing_values_dataset.csv', index=False)\n",
    "\n",
    "print(\"\\nUpdated dataset saved as 'imputed_missing_values_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77a04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c09562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data-for-cleaning.csv' \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Impute numerical columns with mean\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Impute categorical columns with the most frequent value\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Step 2: Remove outliers using the IQR method\n",
    "def remove_outliers(df, cols):\n",
    "    for col in cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Apply outlier removal to numerical columns\n",
    "data = remove_outliers(data, numerical_cols)\n",
    "\n",
    "# Step 3: Standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Save the cleaned dataset\n",
    "output_file_path = 'cleaned_data.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data cleaned and saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d50f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9837911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Preview:\n",
      "   user_id game_title behavior_name  value   x\n",
      "0        1          A          play      5 NaN\n",
      "1        2          B      purchase      1 NaN\n",
      "2        3          C          play    100 NaN\n",
      "3        4          D      purchase      1 NaN\n",
      "4        4          D      purchase      1 NaN\n",
      "\n",
      "After Removing Duplicates:\n",
      "   user_id game_title behavior_name  value   x\n",
      "0        1          A          play      5 NaN\n",
      "1        2          B      purchase      1 NaN\n",
      "2        3          C          play    100 NaN\n",
      "3        4          D      purchase      1 NaN\n",
      "5        5          E          play    120 NaN\n",
      "\n",
      "After Handling Missing Values:\n",
      "   user_id game_title behavior_name  value   x\n",
      "0        1          A          play      5 NaN\n",
      "1        2          B      purchase      1 NaN\n",
      "2        3          C          play    100 NaN\n",
      "3        4          D      purchase      1 NaN\n",
      "5        5          E          play    120 NaN\n",
      "\n",
      "After Removing Outliers:\n",
      "   user_id game_title behavior_name  value   x\n",
      "0        1          A          play      5 NaN\n",
      "1        2          B      purchase      1 NaN\n",
      "2        3          C          play    100 NaN\n",
      "3        4          D      purchase      1 NaN\n",
      "5        5          E          play    120 NaN\n",
      "\n",
      "Final Cleaned Dataset:\n",
      "   user_id game_title behavior_name  value\n",
      "0        1          A          play      5\n",
      "1        2          B      purchase      1\n",
      "2        3          C          play    100\n",
      "3        4          D      purchase      1\n",
      "5        5          E          play    120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arya Sahane\\AppData\\Local\\Temp\\ipykernel_21572\\437902284.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates[col].fillna(df_no_duplicates[col].mean(), inplace=True)\n",
      "C:\\Users\\Arya Sahane\\AppData\\Local\\Temp\\ipykernel_21572\\437902284.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates[col].fillna(df_no_duplicates[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Arya Sahane\\AppData\\Local\\Temp\\ipykernel_21572\\437902284.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates[col].fillna(df_no_duplicates[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Arya Sahane\\AppData\\Local\\Temp\\ipykernel_21572\\437902284.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates[col].fillna(df_no_duplicates[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# --- Optional: Self-Made Dataset (Uncomment to use) ---\n",
    "data = {\n",
    "     'user_id': [1, 2, 3, 4, 4, 5, 6, 7],\n",
    "     'game_title': ['A', 'B', 'C', 'D', 'D', 'E', 'F', 'G'],\n",
    "     'behavior_name': ['play', 'purchase', 'play', 'purchase', 'purchase', 'play', 'play', 'play'],\n",
    "     'value': [5, 1, 100, 1, 1, 120, 4, 3],\n",
    "     'x': [None] * 8  # Optional unnecessary column\n",
    " }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('steam_sample.csv', index=False)\n",
    "\n",
    "# --- Step 1: Load Dataset ---\n",
    "try:\n",
    "    df = pd.read_csv(\"steam_sample.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please make sure 'steam_sample.csv' is present.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Original Dataset Preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- Step 2: Remove Duplicates ---\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nAfter Removing Duplicates:\")\n",
    "print(df_no_duplicates.head())\n",
    "\n",
    "# --- Step 3: Handle Missing Values ---\n",
    "for col in df_no_duplicates.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_no_duplicates[col]):\n",
    "        df_no_duplicates[col].fillna(df_no_duplicates[col].mean(), inplace=True)\n",
    "    else:\n",
    "        df_no_duplicates[col].fillna(df_no_duplicates[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"\\nAfter Handling Missing Values:\")\n",
    "print(df_no_duplicates.head())\n",
    "\n",
    "# --- Step 4: Handle Outliers in 'value' column ---\n",
    "Q1 = df_no_duplicates['value'].quantile(0.25)\n",
    "Q3 = df_no_duplicates['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df_no_outliers = df_no_duplicates[(df_no_duplicates['value'] >= lower_bound) & (df_no_duplicates['value'] <= upper_bound)]\n",
    "\n",
    "print(\"\\nAfter Removing Outliers:\")\n",
    "print(df_no_outliers.head())\n",
    "\n",
    "# --- Step 5: Drop Unnecessary Columns (e.g., 'x') ---\n",
    "if 'x' in df_no_outliers.columns:\n",
    "    df_no_outliers = df_no_outliers.drop(columns=['x'])\n",
    "\n",
    "print(\"\\nFinal Cleaned Dataset:\")\n",
    "print(df_no_outliers.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
